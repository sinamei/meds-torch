input_dir: ${oc.env:MEDS_DIR}
cohort_dir: ${oc.env:MODEL_DIR}
output_dir: ${oc.env:MODEL_DIR}

stages:
  # Aggregate code metadata for filtering and occluding
  - fit_filter_and_occlude:
      _base_stage: aggregate_code_metadata
      aggregations:
        - code/n_occurrences
        - code/n_subjects
        - values/sum
        - values/sum_sqd
        - values/n_occurrences
  # Filter measurements by code frequency
  - filter_measurements:
      min_subjects_per_code: 10000
  # Remove subjects with too few events or measurements
  - filter_subjects:
      min_events_per_subject: 10
      min_measurements_per_subject: 10
  # Occlude outliers beyond threshold stddev_cutoff (currently no-op as stddev_cutoff not set)
  - occlude_outliers
  # Reorder measurements within each (subject_id, time) event
  - reorder_measurements:
      order: $(python -m meds_torch.utils.get_all_measurements metadata_fp=${output_dir}/metadata/codes.parquet)
  # Recompute code metadata with quantile stats
  - fit_normalization:
      _base_stage: aggregate_code_metadata
      aggregations:
        - code/n_occurrences
        - code/n_subjects
        - name: values/quantiles
          quantiles: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
  # Assign code/vocab_index to each code
  - fit_vocabulary_indices
  # Normalize numeric values and convert code â†’ code/vocab_index (with text support)
  - custom_text_normalization
  # Tokenize into schemas, event sequences, and text modalities
  - text_tokenization:
      output_dir: ${output_dir}/tokenization
  # Convert event sequences to nested ragged tensor format for PyTorch dataloader
  - tensorization:
      data_input_dir: ${output_dir}/tokenization
      output_dir: ${output_dir}/tokenization/data
